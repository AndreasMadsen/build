<!doctype html>
<html lang="en">
  
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport"
          content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link rel="stylesheet" href="static/css/main.css">

    <script src="https://d3js.org/d3.v5.min.js"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/4.7.3/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


    <script src="static/js/typeahead.bundle.js"></script>

    <link rel="stylesheet" href="static/css/typeahead.css">
    <link rel="stylesheet"
          href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
          integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T"
          crossorigin="anonymous">

    <title> ICLR: Learning from Unlabelled Videos Using Contrastive Predictive Neural 3D Mapping </title>
</head>


  <body >

<!-- NAV -->

<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Lato:400,900&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Exo" rel='stylesheet'>
<link href="https://fonts.googleapis.com/css?family=Cuprum" rel='stylesheet'>
<nav class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto " id="main-nav" >
  <div class="container">
    <a class="navbar-brand" href="#">
      <img class="logo" style='visibility: ' src="static/images/ICLR-logo.png"  width="180px" />

    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item ">
          <a class="nav-link" href="index.html">Home</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="papers.html">Browse</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="calendar.html">Schedule</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="events.html">Events</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="sponsors.html">Sponsors</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="paper_vis.html">Extras</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="chat.html">Chat</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="about.html">About</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="faq.html">Help</a>
        </li>
        
      </ul>
    </div>
  </div>
</nav>


<div class="container">

 <!-- Title -->

 <div class="pp-card m-3" style="">
   <div class="card-header">
     <h2 class="card-title main-title text-center" style="">
       Learning from Unlabelled Videos Using Contrastive Predictive Neural 3D Mapping
     </h2>

     <h3 class="card-subtitle mb-2 text-muted text-center">
       
       Adam W. Harley,
       
       Shrinidhi K. Lakshmikanth,
       
       Fangyu Li,
       
       Xian Zhou,
       
       Hsiao-Yu Fish Tung,
       
       Katerina Fragkiadaki
       
     </h3>

     <div class="text-center p-3">
       <a class="card-link" data-toggle="collapse" role="button" href="#details">
           Abstract
       </a>

       <a class="card-link"  target="_blank" href="http://www.openreview.net/pdf/abeec25f410ea4e54959aa68bc7ed0aa36858274.pdf">
           Paper
       </a>
       <a class="card-link"  target="_blank"  href="http://www.openreview.net/forum?id=BJxt60VtPr">
           OpenReview
       </a>

     <!-- </div> -->
     
     <!-- <div class="text-center "> -->
       <a href="" target="_blank"  class="card-link">
           Zoom
       </a>

       <a href="https://iclr.rocket.chat/channel/paper_channel_BJxt60VtPr" target="_blank"  class="card-link">
           Chat
       </a>

       
       <a href="https://github.com/aharley/neural_3d_mapping" target="_blank"  class="card-link">
           Code
       </a>
       

       
     </div>
   </div>

   </div>

 <div id="details" class="pp-card m-3 collapse">
   <div class="card-body">
     <p class="card-text">
       <div id="abstractExample">
         <span class="font-weight-bold">Abstract:</span>
         Predictive coding theories suggest that the brain learns by predicting observations at various levels of abstraction. One of the most basic prediction tasks is view prediction: how would a given scene look from an alternative viewpoint? Humans excel at this task. Our ability to imagine and fill in missing information is tightly coupled with perception: we feel as if we see the world in 3 dimensions, while in fact, information from only the front surface of the world hits our retinas. This paper explores the role of view prediction in the development of 3D visual recognition. We propose neural 3D mapping networks, which take as input 2.5D (color and depth) video streams captured by a moving camera, and lift them to stable 3D feature maps of the scene, by disentangling the scene content from the motion of the camera. The model also projects its 3D feature maps to novel viewpoints, to predict and match against target views. We propose contrastive prediction losses to replace the standard color regression loss, and show that this leads to better performance on complex photorealistic data. We show that the proposed model learns visual representations useful for (1) semi-supervised learning of 3D object detectors, and (2) unsupervised learning of 3D moving object detectors, by estimating the motion of the inferred 3D feature maps in videos of dynamic scenes. To the best of our knowledge, this is the first work that empirically shows view prediction to be a scalable self-supervised task beneficial to 3D object detection. 
       </div>

     </p>

     <p></p>
     <p class="card-text"><span class="font-weight-bold">Keywords:</span>
       
       <a href="keyword_3D feature learning.html" class="text-secondary text-decoration-none">3D feature learning</a>,
       
       <a href="keyword_unsupervised learning.html" class="text-secondary text-decoration-none">unsupervised learning</a>,
       
       <a href="keyword_inverse graphics.html" class="text-secondary text-decoration-none">inverse graphics</a>,
       
       <a href="keyword_object discovery.html" class="text-secondary text-decoration-none">object discovery</a>
       
     </p>
   </div>
 </div>

</div>

<!-- SlidesLive -->



 <div class="container" style="background-color:white; padding: 0px;">
  <div class="row m-2">
    <div class="col-md-7 col-xs-12 my-auto p-2" >
      <div id="presentation-embed-38915748" class="slp my-auto"></div>
      <script src='https://slideslive.com/embed_presentation.js'></script>
      <script>
        embed = new SlidesLiveEmbed('presentation-embed-38915748', {
        presentationId: '38915748',
        autoPlay: false, // change to true to autoplay the embedded presentation
        verticalEnabled: true,
        verticalWhenWidthLte: 2000,
        allowHiddenControlsWhenPaused: true,
        hideTitle: true
        });
      </script>
    </div>

    <div class="col-md-5 col-xs-12 p-2">
        <div id="gitter" class="slp">
          <center>
             <iframe frameborder="0" src="https://iclr.rocket.chat/channel/paper_channel_BJxt60VtPr?layout=embedded" height="700px" width="100%" ></iframe>
          </center>
        </div>
      </div>
    </div>
  </div>
</div>

  <!-- Recs -->
  <p></p>


  <div  class="container" style="padding-bottom: 30px; padding-top:30px">
  <center>
    <h2> Similar Papers </h2>
</div>
<p></p>

<div  class="container" >
  <div class="row">

      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_rkl03ySYDH.html" class="text-muted">
              <h5 class="card-title" align="center">SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Zhixuan Lin,
              
              Yi-Fu Wu,
              
              Skand Vishwanath Peri,
              
              Weihao Sun,
              
              Gautam Singh,
              
              Fei Deng,
              
              Jindong Jiang,
              
              Sungjin Ahn,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/rkl03ySYDH.png" width="80%"/></center>

            <!-- <p class="card-text"> We propose a generative latent variable model for unsupervised scene decomposition that provides factorized object representation per foreground object while also decomposing background segments of complex morphology.</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_SkeyppEFvS.html" class="text-muted">
              <h5 class="card-title" align="center">CoPhy: Counterfactual Learning of Physical Dynamics</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Fabien Baradel,
              
              Natalia Neverova,
              
              Julien Mille,
              
              Greg Mori,
              
              Christian Wolf,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/SkeyppEFvS.png" width="80%"/></center>

            <!-- <p class="card-text"> Understanding causes and effects in mechanical systems is an essential component of reasoning in the physical world. This work poses a new problem of counterfactual learning of object mechanics from visual input.  We develop the CoPhy benchmark to as...</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_Hyg9anEFPS.html" class="text-muted">
              <h5 class="card-title" align="center">Image-guided Neural Object Rendering</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Justus Thies,
              
              Michael Zollhöfer,
              
              Christian Theobalt,
              
              Marc Stamminger,
              
              Matthias Nießner,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/Hyg9anEFPS.png" width="80%"/></center>

            <!-- <p class="card-text"> We propose a learned image-guided rendering technique that combines the benefits of image-based rendering and GAN-based image synthesis while considering view-dependent effects.</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_BkxfaTVFwH.html" class="text-muted">
              <h5 class="card-title" align="center">GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Martin Engelcke,
              
              Adam R. Kosiorek,
              
              Oiwi Parker Jones,
              
              Ingmar Posner,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/BkxfaTVFwH.png" width="80%"/></center>

            <!-- <p class="card-text"> We present the first object-centric generative model of 3D visual scenes capable of both decomposing and generating scenes.</p> -->

          </div>
        </div>
      </div>
      
  </DIV>
</DIV>

</body>