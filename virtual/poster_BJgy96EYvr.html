<!doctype html>
<html lang="en">
  
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport"
          content="width=device-width, initial-scale=1, shrink-to-fit=no">



    <script src="https://d3js.org/d3.v5.min.js"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/4.7.3/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


    <script src="static/typeahead.bundle.js"></script>

    <link rel="stylesheet" href="static/typeahead.css">
    <link rel="stylesheet"
          href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
          integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T"
          crossorigin="anonymous">

    <title> ICLR: Influence-Based Multi-Agent Exploration </title>
</head>


  <body >

<!-- NAV -->

<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Lato:400,900&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Exo" rel='stylesheet'>
<link href="https://fonts.googleapis.com/css?family=Cuprum" rel='stylesheet'>

<style>
  body{font-family: 'Lato', sans-serif; background-color: rgba(236, 241, 246, 1)}
  .jumbotron{font-family: 'Lato', sans-serif; background-color: rgba(236, 241, 246, 1)}
  .btn-group{background-color: white}
  .btn {background-color: white}
  #main-nav {padding-top:10px; padding-bottom:10px;}
  .card {font-family: 'Exo'; box-shadow: 2px 2px 14px 0px rgba(204, 204, 204, 1);}
  .header {font: "Montserrat"; }
  .card-header { border: 4px solid #eee;
                font-family: "Exo";}

  
  .main-title {font-weight: 700;  color: #2294e0;}

  .myAccordion {

  box-shadow: 0px 2px 14px 0px rgba(0, 0, 0, 0.10);
    border-radius: 10px;
    margin-bottom: 18px;
    padding-left: 15px;
    padding-bottom: 10px;
    padding-right: 15px;
    padding-top: 10px;
    background-color: rgba(255, 255, 255, 1);
  }

  .sponsorLogo {
  width: 250px;
  display: block;
  margin-left: auto;
    margin-right: auto;
  margin-top: auto;
    margin-bottom: auto;

  }
  
  .slp {

  background: #fff

  padding: 10px;

  border: 4px solid #eee;

  box-shadow: rgb(204, 204, 204) 2px 2px 14px 0px;
  }
  
  .border {
  background: #fff
  
  padding: 10px;

  border: 4px solid #eee;

  box-shadow: rgb(204, 204, 204) 2px 2px 14px 0px;
  }
  
  #abstractExample.collapse:not(.show) {
  display: block;
  /* height = lineheight * no of lines to display */
  height: 4.5em;
  overflow: hidden;
  }

  #abstractExample.collapsing {
  height: 4.5em;
  }


#absShow.collapsed:after {
  content: '+ Show More';
}

#absShow:not(.collapsed):after {
  content: '- Show Less';
}
</style>


<nav class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto " id="main-nav" >
  <div class="container">
    <a class="navbar-brand" href="#">
      <img class="logo" style='visibility: ' src="static/ICLR-logo.png"  width="180px" />

    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item ">
          <a class="nav-link" href="index.html">Home</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="livestream.html">Live</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="papers.html">Papers</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="paper_vis.html">Vis</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="calendar.html">Schedule</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="socials.html">Socials</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="speakers.html">Speakers</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="sponsors.html">Sponsors</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="workshops.html">Workshops</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="faq.html">FAQ</a>
        </li>
        
      </ul>
    </div>
  </div>
</nav>


<div class="container">

 <!-- Title -->

 <div class="card m-3" style="">
   <div class="card-header">
     <h3 class="card-title main-title text-center" style="">
       Influence-Based Multi-Agent Exploration
     </h3>
     
     <h5 class="card-subtitle mb-2 text-muted text-center">
       
       Tonghan Wang*,
       
       Jianhao Wang*,
       
       Yi Wu,
       
       Chongjie Zhang
       
     </h5>
     
     <center class="p-3">
       <a class="card-link" data-toggle="collapse" role="button" href="#details">
         <button class="btn btn-outline-secondary">
           Abstract
         </button>
       </a>
       

       <a class="card-link"  target="_blank" href="http://www.openreview.net/pdf/ed71e74b6d27183d7d2c1f422be6180d56ea7698.pdf">
         <button class="btn btn-outline-secondary">
           Paper
         </button>
       </a>
       <a class="card-link"  target="_blank"  href="http://www.openreview.net/forum?id=BJgy96EYvr">
         <button class="btn btn-outline-secondary">
           OpenReview
         </button>
       </a>
       
       <a href="" target="_blank"  class="card-link">
         <button class="btn btn-outline-secondary">
           Zoom
         </button>
       </a>
       
       <a href="https://github.com/TonghanWang/EITI-EDTI" target="_blank"  class="card-link">
         <button class="btn btn-outline-secondary">
           Code
         </button>
       </a>
       
       <a href="" target="_blank"  class="card-link">
         <button class="btn btn-outline-secondary">
           Slides
         </button>
       </a>
     </center>
     
   </div>
 </div>

 <div id="details" class="card m-3 collapse" style=" box-shadow: 2px 2px 14px 0px rgba(204, 204, 204, 1);">
   <div class="card-body">
     <p class="card-text">
       <div id="abstractExample">
         <span class="font-weight-bold">Abstract:</span>
         Intrinsically motivated reinforcement learning aims to address the exploration challenge for sparse-reward tasks. However, the study of exploration methods in transition-dependent multi-agent settings is largely absent from the literature. We aim to take a step towards solving this problem. We present two exploration methods: exploration via information-theoretic influence (EITI) and exploration via decision-theoretic influence (EDTI), by exploiting the role of interaction in coordinated behaviors of agents. EITI uses mutual information to capture the interdependence between the transition dynamics of agents. EDTI uses a novel intrinsic reward, called Value of Interaction (VoI), to characterize and quantify the influence of one agent&#39;s behavior on expected returns of other agents. By optimizing EITI or EDTI objective as a regularizer, agents are encouraged to coordinate their exploration and learn policies to optimize the team performance. We show how to optimize these regularizers so that they can be easily integrated with policy gradient reinforcement learning. The resulting update rule draws a connection between coordinated exploration and intrinsic reward distribution. Finally, we empirically demonstrate the significant strength of our methods in a variety of multi-agent scenarios.
       </div>
       
     </p>
     
     <p></p>
     <p class="card-text"><span class="font-weight-bold">Keywords:</span>
       
       <a href="keyword_Multi-agent reinforcement learning.html" class="text-secondary text-decoration-none">Multi-agent reinforcement learning</a>,
       
       <a href="keyword_Exploration.html" class="text-secondary text-decoration-none">Exploration</a>
       
     </p>
   </div>
 </div>

</div>

<!-- SlidesLive -->



<div class="container ">
  <div class="row m-2">
    <div class="col-7">
      <div id="presentation-embed-38915748" class="slp"></div>
      <script src='https://slideslive.com/embed_presentation.js'></script>
      <script>
        embed = new SlidesLiveEmbed('presentation-embed-38915748', {
        presentationId: '38915748',
        autoPlay: false, // change to true to autoplay the embedded presentation
        verticalEnabled: true,
        allowHiddenControlsWhenPaused: true,
        zoomRatio: 0.4,
        hideTitle: true
        });
      </script>
    </div>
    
    <div class="col-5 p-1 my-auto">        
        <div id="gitter" class="gitter container " >
          <center>
            <div class="border">
              <center> <iframe frameborder="0" src="https://iclr.rocket.chat/channel/paper_channel_BJgy96EYvr?layout=embedded" width="100%" height="680px"></iframe> </center>
            </div>
          </center>
        </div>
      </div>
    </div>
  </div>
</div>

  <!-- Recs -->
  <p></p>


  <div  class="container" style="padding-bottom: 30px; padding-top:30px">
  <center>
    <h2> Similar Papers </h2>
</div>
<p></p>

<div  class="container" >
  <div class="row">
  

      
      <div class="col-3">
        <div class="card" >
          <div class="card-header text-center">
            <a href="poster_rkg-TJBFPB.html" class="text-dark"><h5 class="card-title">RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments</h5></a>
            <center><img src="https://iclr.github.io/iclr-images/rkg-TJBFPB.png" width="75%"  style="margin-bottom: 20px; margin-top: 20px; border-radius: 0; border: 4px solid #eee;box-shadow: 2px 2px 8px 0 #ccc;"/></center>

            <p class="card-text"> Reward agents for taking actions that lead to changes in the environment state.</p>
            
          </div>      
        </div>
      </div>
      
      <div class="col-3">
        <div class="card" >
          <div class="card-header text-center">
            <a href="poster_Sye57xStvB.html" class="text-dark"><h5 class="card-title">Never Give Up: Learning Directed Exploration Strategies</h5></a>
            <center><img src="https://iclr.github.io/iclr-images/Sye57xStvB.png" width="75%"  style="margin-bottom: 20px; margin-top: 20px; border-radius: 0; border: 4px solid #eee;box-shadow: 2px 2px 8px 0 #ccc;"/></center>

            <p class="card-text"> We propose a reinforcement learning agent to solve hard exploration games by learning a range of directed exploratory policies. </p>
            
          </div>      
        </div>
      </div>
      
      <div class="col-3">
        <div class="card" >
          <div class="card-header text-center">
            <a href="poster_BJewlyStDr.html" class="text-dark"><h5 class="card-title">On Bonus Based Exploration Methods In The Arcade Learning Environment</h5></a>
            <center><img src="https://iclr.github.io/iclr-images/BJewlyStDr.png" width="75%"  style="margin-bottom: 20px; margin-top: 20px; border-radius: 0; border: 4px solid #eee;box-shadow: 2px 2px 8px 0 #ccc;"/></center>

            <p class="card-text"> We find that existing bonus-based exploration methods have not been able to address the exploration-exploitation trade-off in the Arcade Learning Environment. </p>
            
          </div>      
        </div>
      </div>
      
      <div class="col-3">
        <div class="card" >
          <div class="card-header text-center">
            <a href="poster_S1lEX04tPr.html" class="text-dark"><h5 class="card-title">CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning</h5></a>
            <center><img src="https://iclr.github.io/iclr-images/S1lEX04tPr.png" width="75%"  style="margin-bottom: 20px; margin-top: 20px; border-radius: 0; border: 4px solid #eee;box-shadow: 2px 2px 8px 0 #ccc;"/></center>

            <p class="card-text"> A modular method for fully cooperative multi-goal multi-agent reinforcement learning, based on curriculum learning for efficient exploration and credit assignment for action-goal interactions.</p>
            
          </div>      
        </div>
      </div>
      
  </DIV>
</DIV>

</body>