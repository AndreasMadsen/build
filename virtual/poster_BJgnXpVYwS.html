<!doctype html>
<html lang="en">
  
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport"
          content="width=device-width, initial-scale=1, shrink-to-fit=no">



    <script src="https://d3js.org/d3.v5.min.js"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/4.7.3/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


    <script src="static/typeahead.bundle.js"></script>

    <link rel="stylesheet" href="static/typeahead.css">
    <link rel="stylesheet"
          href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
          integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T"
          crossorigin="anonymous">

    <title> ICLR: Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity </title>
</head>


  <body >

<!-- NAV -->

<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Lato:400,900&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Exo" rel='stylesheet'>
<link href="https://fonts.googleapis.com/css?family=Cuprum" rel='stylesheet'>

<style>
  body{font-family: 'Lato', sans-serif; background-color: rgba(236, 241, 246, 1)}
  .btn-group{background-color: white}
  .btn {background-color: white}
  #main-nav {padding-top:10px; padding-bottom:10px;}
  .card {font-family: 'Exo'; box-shadow: 2px 2px 14px 0px rgba(204, 204, 204, 1);}
  .header {font: "Montserrat"; }
  .card-header { border: 4px solid #eee;
                font-family: "Exo";}


  .main-title {font-weight: 700;  color: #2294e0;}

  .myAccordion {

  box-shadow: 0px 2px 14px 0px rgba(0, 0, 0, 0.10);
    border-radius: 10px;
    margin-bottom: 18px;
    padding-left: 15px;
    padding-bottom: 10px;
    padding-right: 15px;
    padding-top: 10px;
    background-color: rgba(255, 255, 255, 1);
  }

  #abstractExample.collapse:not(.show) {
  display: block;
  /* height = lineheight * no of lines to display */
  height: 4.5em;
  overflow: hidden;
  }

  #abstractExample.collapsing {
  height: 4.5em;
  }


#absShow.collapsed:after {
  content: '+ Show More';
}

#absShow:not(.collapsed):after {
  content: '- Show Less';
}
</style>


<nav class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto " id="main-nav" >
  <div class="container">
    <a class="navbar-brand" href="#">
      <img class="logo" style='visibility: ' src="static/ICLR-logo.png"  width="180px" />

    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item ">
          <a class="nav-link" href="index.html">Home</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="livestream.html">Live</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="papers.html">Papers</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="paper_vis.html">Vis</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="calendar.html">Schedule</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="socials.html">Socials</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="speakers.html">Speakers</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="sponsors.html">Sponsors</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="workshops.html">Workshops</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="faq.html">FAQ</a>
        </li>
        
      </ul>
    </div>
  </div>
</nav>


<div class="container">

 <!-- Title -->

<div class="card m-3" style="">
  <div class="card-header">
    <h3 class="card-title main-title text-center" style="">
      Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity
    </h3>

    <h5 class="card-subtitle mb-2 text-muted text-center">
      
      Jingzhao Zhang,
      
      Tianxing He,
      
      Suvrit Sra,
      
      Ali Jadbabaie
      
    </h5>

    <center class="p-3">
      <a class="card-link" data-toggle="collapse" role="button" href="#details">
        <button class="btn btn-outline-secondary">
          Abstract
        </button>
      </a>


      <a class="card-link"  target="_blank" href="http://www.openreview.net/pdf/3b53be5599473dd2d2275e3acaa1e2890cad4e1e.pdf">
        <button class="btn btn-outline-secondary">
          Paper
        </button>
      </a>
      <a class="card-link"  target="_blank"  href="http://www.openreview.net/forum?id=BJgnXpVYwS">
        <button class="btn btn-outline-secondary">
          OpenReview
        </button>
      </a>
    <!-- <span><a href="" class="btn btn-secondary">OpenReview</a></span> -->

    <a href="https://github.com/JingzhaoZhang/why-clipping-accelerates" target="_blank"  class="card-link">
      <button class="btn btn-outline-secondary">
        Code
      </button>
    </a>

    <a href="" target="_blank"  class="card-link">
      <button class="btn btn-outline-secondary">
        Slides
      </button>
    </a>
    </center>

  </div>
</div>

<div id="details" class="card m-3 collapse" style="font-family: 'Cuprum'; box-shadow: 2px 2px 14px 0px rgba(204, 204, 204, 1);">
  <div class="card-body">
    <p class="card-text">
      <div id="abstractExample">
        <span class="font-weight-bold">Abstract:</span>
        We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.
      </div>

    </p>

    <p></p>
    <p class="card-text"><span class="font-weight-bold">Keywords:</span>
      
      <a href="keyword_Adaptive methods.html" class="text-secondary text-decoration-none">Adaptive methods</a>,
      
      <a href="keyword_optimization.html" class="text-secondary text-decoration-none">optimization</a>,
      
      <a href="keyword_deep learning.html" class="text-secondary text-decoration-none">deep learning</a>
      
    </p>
  </div>
</div>

<div>
</div>

</div>


<!-- SlidesLive -->
<div id="presentation-embed-38915748" class="container container-sm"></div>
<script src='https://slideslive.com/embed_presentation.js'></script>
<script>
  embed = new SlidesLiveEmbed('presentation-embed-38915748', {
        presentationId: '38915748',
        autoPlay: false, // change to true to autoplay the embedded presentation
        verticalEnabled: true,
        allowHiddenControlsWhenPaused: true,
        zoomRatio: 0.4,
        hideTitle: true
    });
</script>


<!-- Buttons -->
<div class="container" style="padding-bottom: 30px; padding-top:30px">
  <center>
    <h2> Paper Discussion       </h2>
    <!-- <span><a class="btn btn-secondary" href="https://gitter.im/iclr/posterBJgnXpVYwS/">Chat</a></span> -->
  </center>
  <p></p>

  <!-- Gitter -->
  <!-- <div id='discourse-comments'></div> -->


<!-- <script type="text/javascript"> -->
<!--   DiscourseEmbed = { discourseUrl: 'https://iclr.trydiscourse.com/', -->
<!--                      topicId: 20}; -->

<!--   (function() { -->
<!--     var d = document.createElement('script'); d.type = 'text/javascript'; d.async = true; -->
<!--     d.src = DiscourseEmbed.discourseUrl + 'javascripts/embed.js'; -->
<!--     (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(d); -->
<!--   })(); -->
<!-- </script> -->


  <div id="gitter" class="gitter container" height="600px">
    <center>
      <div class="border">

        <center> <iframe frameborder="0" src="https://iclr.rocket.chat/channel/paper_channel_BJgnXpVYwS?layout=embedded" width="900px" height="400px"></iframe> </center>
      </div>
    </center>
  </div>
</div>


  <!-- Recs -->
  <p></p>


  <div  class="container" style="padding-bottom: 30px; padding-top:30px">
  <center>
    <h2> Similar Papers </h2>
</div>
<p></p>

<div  class="container" >
  <div class="row">
    <div class="card-deck">

      

  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_rklB76EKPr.html" class="text-dark"><h5 class="card-title">Can gradient clipping mitigate label noise?</h5></a>

    </div>
      <div class="card-body">
        <p class="card-text"> Gradient clipping doesn&#39;t endow robustness to label noise, but a simple loss-based variant does.</p>
      </div>

      <div class="card-footer">
      <center>
        <a href="poster_rklB76EKPr.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  

  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_ryeFY0EFwS.html" class="text-dark"><h5 class="card-title">Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization</h5></a>

    </div>
      <div class="card-body">
        <p class="card-text"> We propose a hypothesis for why gradient descent generalizes based on how per-example gradients interact with each other.</p>
      </div>

      <div class="card-footer">
      <center>
        <a href="poster_ryeFY0EFwS.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  

  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_SJeLIgBKPS.html" class="text-dark"><h5 class="card-title">Gradient Descent Maximizes the Margin of Homogeneous Neural Networks</h5></a>

    </div>
      <div class="card-body">
        <p class="card-text"> We study the implicit bias of gradient descent and prove under a minimal set of assumptions that the parameter direction of homogeneous models converges to KKT points of a natural margin maximization problem.</p>
      </div>

      <div class="card-footer">
      <center>
        <a href="poster_SJeLIgBKPS.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  

  <!-- <div class="col-sm-4" style="padding-bottom: 10px"> -->
  <div class="card" >
    <div class="card-header">
        <a href="poster_rkg1ngrFPr.html" class="text-dark"><h5 class="card-title">Information Geometry of Orthogonal Initializations and Training</h5></a>

    </div>
      <div class="card-body">
        <p class="card-text"> nearly isometric DNN initializations imply low parameter space curvature, and a lower condition number, but that&#39;s not always great</p>
      </div>

      <div class="card-footer">
      <center>
        <a href="poster_rkg1ngrFPr.html" class="btn btn-primary">Visit</a>
        </center>

      </div>
      <!-- </div> -->

  </div>
  
    </DIV>
          </DIV>
      </DIV>

</body>