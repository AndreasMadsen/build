<!doctype html>
<html lang="en">
  
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport"
          content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link rel="stylesheet" href="static/css/main.css">

    <script src="https://d3js.org/d3.v5.min.js"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/4.7.3/handlebars.min.js"
            integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU="
            crossorigin="anonymous"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"
            integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
            crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


    <script src="static/js/typeahead.bundle.js"></script>

    <link rel="stylesheet" href="static/css/typeahead.css">
    <link rel="stylesheet"
          href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
          integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T"
          crossorigin="anonymous">

    <title> ICLR: Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks </title>
</head>


  <body >

<!-- NAV -->

<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Lato:400,900&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Exo" rel='stylesheet'>
<link href="https://fonts.googleapis.com/css?family=Cuprum" rel='stylesheet'>
<nav class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto " id="main-nav" >
  <div class="container">
    <a class="navbar-brand" href="index.html">
      <img class="logo" style='visibility: ' src="static/images/ICLR-logo.png"  width="180px" />

    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item ">
          <a class="nav-link" href="index.html">Home</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="papers.html">Browse</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="calendar.html">Schedule</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="events.html">Events</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="sponsors.html">Booths</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="paper_vis.html">Vis</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="chat.html">Chat</a>
        </li>
        
        <li class="nav-item ">
          <a class="nav-link" href="about.html">About</a>
        </li>
        
      </ul>
    </div>
  </div>
</nav>


<div class="container">

 <!-- Title -->

 <div class="pp-card m-3" style="">
   <div class="card-header">
     <h2 class="card-title main-title text-center" style="">
       Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks
     </h2>

     <h3 class="card-subtitle mb-2 text-muted text-center">
       
       Yuhang Li,
       
       Xin Dong,
       
       Wei Wang
       
     </h3>

     <div class="text-center p-3">
       <a class="card-link" data-toggle="collapse" role="button" href="#details">
           Abstract
       </a>

       <a class="card-link"  target="_blank" href="http://www.openreview.net/pdf/65bbe05758ea9345981db8c060d6b183847f4ced.pdf">
           Paper
       </a>
       <a class="card-link"  target="_blank"  href="http://www.openreview.net/forum?id=BkgXT24tDS">
           OpenReview
       </a>

     <!-- </div> -->
     
     <!-- <div class="text-center "> -->
       <a href="" target="_blank"  class="card-link">
           Zoom
       </a>

       <a href="https://iclr.rocket.chat/channel/paper_channel_BkgXT24tDS" target="_blank"  class="card-link">
           Chat
       </a>

       
       <a href="https://github.com/yhhhli/APoT_Quantization" target="_blank"  class="card-link">
           Code
       </a>
       

       
     </div>
   </div>

   </div>

 <div id="details" class="pp-card m-3 collapse">
   <div class="card-body">
     <p class="card-text">
       <div id="abstractExample">
         <span class="font-weight-bold">Abstract:</span>
         We propose Additive Powers-of-Two~(APoT) quantization, an efficient non-uniform quantization scheme for the bell-shaped and long-tailed distribution of weights and activations in neural networks. By constraining all quantization levels as the sum of Powers-of-Two terms, APoT quantization enjoys high computational efficiency and a good match with the distribution of weights. A simple reparameterization of the clipping function is applied to generate a better-defined gradient for learning the clipping threshold. Moreover, weight normalization is presented to refine the distribution of weights to make the training more stable and consistent. Experimental results show that our proposed method outperforms state-of-the-art methods, and is even competitive with the full-precision models, demonstrating the effectiveness of our proposed APoT quantization. For example, our 4-bit quantized ResNet-50 on ImageNet achieves 76.6% top-1 accuracy without bells and whistles; meanwhile, our model reduces 22% computational cost compared with the uniformly quantized counterpart.
       </div>

     </p>

     <p></p>
     <p class="card-text"><span class="font-weight-bold">Keywords:</span>
       
       <a href="papers.html?filter=keywords&search=Quantization" class="text-secondary text-decoration-none">Quantization</a>,
       
       <a href="papers.html?filter=keywords&search=Efficient Inference" class="text-secondary text-decoration-none">Efficient Inference</a>,
       
       <a href="papers.html?filter=keywords&search=Neural Networks" class="text-secondary text-decoration-none">Neural Networks</a>
       
     </p>
   </div>
 </div>

</div>

<!-- SlidesLive -->



 <div class="container" style="background-color:white; padding: 0px;">
  <div class="row m-2">
    <div class="col-md-7 col-xs-12 my-auto p-2" >
      <div id="presentation-embed-38915748" class="slp my-auto"></div>
      <script src='https://slideslive.com/embed_presentation.js'></script>
      <script>
        embed = new SlidesLiveEmbed('presentation-embed-38915748', {
        presentationId: '38915748',
        autoPlay: false, // change to true to autoplay the embedded presentation
        verticalEnabled: true,
        verticalWhenWidthLte: 2000,
        allowHiddenControlsWhenPaused: true,
        hideTitle: true
        });
      </script>
    </div>

    <div class="col-md-5 col-xs-12 p-2">
        <div id="gitter" class="slp">
          <center>
             <iframe frameborder="0" src="https://iclr.rocket.chat/channel/paper_channel_BkgXT24tDS?layout=embedded" height="700px" width="100%" ></iframe>
          </center>
        </div>
      </div>
    </div>
  </div>
</div>

  <!-- Recs -->
  <p></p>


  <div  class="container" style="padding-bottom: 30px; padding-top:30px">
  <center>
    <h2> Similar Papers </h2>
</div>
<p></p>

<div  class="container" >
  <div class="row">

      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_H1lBj2VFPS.html" class="text-muted">
              <h5 class="card-title" align="center">Linear Symmetric Quantization of Neural Networks for Low-precision Integer Hardware</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Xiandong Zhao,
              
              Ying Wang,
              
              Xuyi Cai,
              
              Cheng Liu,
              
              Lei Zhang,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/H1lBj2VFPS.png" width="80%"/></center>

            <!-- <p class="card-text"> We introduce an efficient quantization process that allows for performance acceleration on specialized integer-only neural network accelerator.</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_ryxK0JBtPr.html" class="text-muted">
              <h5 class="card-title" align="center">Gradient $\ell_1$ Regularization for Quantization Robustness</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Milad Alizadeh,
              
              Arash Behboodi,
              
              Mart van Baalen,
              
              Christos Louizos,
              
              Tijmen Blankevoort,
              
              Max Welling,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/ryxK0JBtPr.png" width="80%"/></center>

            <!-- <p class="card-text"> We show that regularizing the $\ell_1$-norm of gradients improves robustness to post-training quantization in neural networks.</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_rJehVyrKwH.html" class="text-muted">
              <h5 class="card-title" align="center">And the Bit Goes Down: Revisiting the Quantization of Neural Networks</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Pierre Stock,
              
              Armand Joulin,
              
              Rémi Gribonval,
              
              Benjamin Graham,
              
              Hervé Jégou,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/rJehVyrKwH.png" width="80%"/></center>

            <!-- <p class="card-text"> Using a structured quantization technique aiming at better in-domain reconstruction to compress convolutional neural networks</p> -->

          </div>
        </div>
      </div>
      
      <div class="col-md-4 col-xs-6">
        <div class="pp-card" >
          <div class="pp-card-header" class="text-muted">
            <a href="poster_rkgO66VKDS.html" class="text-muted">
              <h5 class="card-title" align="center">LEARNED STEP SIZE QUANTIZATION</h5></a>
            <h6 class="card-subtitle text-muted" align="center">
              
              Steven K. Esser,
              
              Jeffrey L. McKinstry,
              
              Deepika Bablani,
              
              Rathinakumar Appuswamy,
              
              Dharmendra S. Modha,
              
            </h6>

            <center><img class="cards_img" src="https://iclr.github.io/iclr-images/rkgO66VKDS.png" width="80%"/></center>

            <!-- <p class="card-text"> A method for learning quantization configuration for low precision networks that achieves state of the art performance for quantized networks.</p> -->

          </div>
        </div>
      </div>
      
  </DIV>
</DIV>

</body>